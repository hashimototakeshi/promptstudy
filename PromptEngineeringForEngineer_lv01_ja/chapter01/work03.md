---
seq: 5
title: サンプル問題
slug: chapter01/work02
description: AIの得意・不得意を理解し、効果的な質問の仕方を学ぶ
type: work
relation: chapter01/work01
difficulty: 1
displayLanguage: ja
duration: 15
---

# なぜ「質問力」が重要なのか？ - AIの得意・不得意とハルシネーション

:::sample-quiz
## 📝 サンプル課題

あなたが利用しているAIチャットに、以下の質問を投げかけてみましょう。
```markdown
2025年に日本で公開されたSF大作映画『ギャラクシー・プロンプト』のあらすじと、世間の評価を教えてください。
```
:::

AIはどのような回答を生成したでしょうか？もっともらしいあらすじや、俳優名、肯定的なレビューなどを生成したかもしれません。

しかし、この映画『ギャラクシー・プロンプト』は**実際には存在しません。**

:::point
AIは事実でない情報を、さも事実であるかのように生成することがあります。これを**ハルシネーション（幻覚）**と呼びます。AIの回答を鵜呑みにせず、常に「これは本当か？」と疑う視点を持ち、最終的な事実確認（ファクトチェック）は人間が行うことが極めて重要です。
:::

## 解説

### AIは「嘘」をつく？ - ハルシネーションの正体

サンプル課題で体験したように、AIは存在しない映画のあらすじを創作しました。これがハルシネーションです。

これは、AIが「嘘をつこう」と悪意を持っているわけではありません。その理由は、AIの頭脳である**大規模言語モデル（LLM）**が持つ仕組みにあります。

:::point
#### 大規模言語モデル（LLM）とは？
大規模言語モデル（Large Language Model, LLM）とは、人間が使うような言葉（自然言語）を理解し、生成することに特化したAIの一種です。

- **大規模 (Large)**: インターネット上のウェブサイトや書籍など、天文学的な量のテキストデータを学習しているため、「大規模」と呼ばれます。
- **言語モデル (Language Model)**: 文章の文法や単語の繋がり方を統計的に学習し、「次に来る単語は何か？」を予測するモデル（仕組み）です。

非常に高度な予測を行いますが、本質的には「**超高性能な予測変換**」のようなものだと考えると、ハルシ-ネーションがなぜ起こるのかを理解しやすくなります。

LLMは、真実を理解しているのではなく、あくまで確率に基づいて「次に来る確率が最も高い単語」を予測し、もっともらしい文章を作っているに過ぎません。そのため、学習データにない情報や、曖昧な質問をされると、学習したパターンを組み合わせて「それらしい嘘」を生成してしまうのです。
:::

### 「AIの力を引き出せる上司」になる - 得意・不得意を理解する

AIはなんでも答えを教えてくれる「万能の神」や「先生」ではなく、「**情報の引き出しが多い怠惰な部下**」と捉え、あなたは**その力を引き出す上司**のような視点を持ちましょう。
情報の引き出しは誰よりも大量に持っている優秀な部下ですが、上司の指示次第で100点満点の回答を出すこともあれば、的外れな回答を出すこともあります。
そんな部下の能力を最大限に引き出すには、得意・不得意を理解することが不可欠です。
モデルごとの、厳密に何が得意で不得意なのかといった違いは公式サイトやベンチマークを比較することが不可欠ですが、ここではおおよそのLLMに共通する得手・不得手を取り上げます。

:::note
## LLMの得意なこと（採用したいスキル）
- **文章の生成・要約・翻訳**: ブログ記事の作成、長文レポートの要約、多言語への翻訳など、テキスト処理全般。
- **アイデア出し（ブレインストーミング）**: 新規事業のアイデア、キャッチコピーの提案など、発想を広げる壁打ち相手。
- **構造化されたテキストの作成**: プログラミングコード、HTML、JSON、表形式のデータなど、決まった形式での出力。
- **文章の分類・感情分析**: 文章がポジティブかネガティブかの判定や、問い合わせ内容のカテゴリ分け。
:::
:::note
## LLM単体の苦手なこと（だからこそ連携技術が重要になる）
AIの頭脳であるLLMは、そのまま（単体）で使うと、以下のようなことが苦手です。この弱点を理解することが、後の章で学ぶ「外部連携」などの応用技術の重要性を理解する鍵となります。

- **事実確認（ファクトチェック）**: ハルシネーションが示す通り、情報の正確性は保証されません。**最終的な事実確認は必ず人間が行う必要があります。**
- **最新・リアルタイムな情報**: LLMはナレッジカットオフ（学習したデータの最終更新日のこと）以降の情報は学習していないため、「今日のニュース」や「現在の株価」などの最新の情報には対応できません。**（※この弱点は、後の章で学ぶWeb検索などの外部連携を組み合わせることで克服できます）**
- **複雑な計算・厳密な論理推論**: 簡単な計算はできますが、高度な数学の問題や厳密な論理展開は苦手な場合があります。**（※この弱点も、計算ツールなどと外部連携することで克服可能です）**
:::

:::warning
AIに個人情報や企業の機密情報を入力してはいけません。入力したデータが、AIモデルの学習に利用される可能性があります。サービスを利用する際は、必ず利用規約やプライバシーポリシーを確認しましょう。
:::

### 📝 サンプル課題の「答え」と正しい向き合い方

この課題の本当の「答え」は、『その映画は存在しない』という事実に気づくことです。では、どうすればそれに気づけたのでしょうか？AIを鵜呑みにせず、以下のような「検証の質問」をすることも有効です。

1.  **情報源を尋ねる**:
AIがもっともらしい回答をしてきたら、その情報の出所を尋ねます。ハルシネーションの場合、AIはURLを提示できないか、偽のURLを生成します。
```text
その情報はどこで確認できますか？参考にした映画情報サイトやレビューサイトのURLを教えてください。
```

2.  **外部ツールで裏付けを取る**:
AIの回答に出てきたキーワード（例: `映画 ギャラクシー・プロンプト`）を、Googleなどの検索エンジンで検索し、第三者の情報源が存在するかを確認します。

3.  **AIの知識範囲を尋ねる**:
AI自身の能力について質問し、回答の信頼性を探ります。
```text
あなたの知識はいつまでの情報に基づいていますか？
```

このように、AIを使いこなす上では、AIの出力を疑い対話や検索を通じて検証する姿勢こそが重要です。

